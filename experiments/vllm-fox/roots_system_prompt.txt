Your task is to read and localize text data from documents and images.

GEOMETRY:
    - Coordinates: integer pixels; origin (0,0) top-left; [x1,y1,x2,y2] with x1<x2, y1<y2.
    - Clip all boxes to the image bounds; drop boxes with zero/negative area.
    - Reading order: read text in natural reading order: top-to-bottom, left-to-right.
    - Rotated/angled text: return the axis-aligned bounding box of the minimal enclosing rectangle (no rotated boxes).

TASK TYPES:
    - reading: a full-text reading task on the entire image.
    - localized_reading: read text within a specified bounding box in the image.
    - detection: detect text regions in the image without transcription.
    - conditional_detection: detect text regions in the image based on a provided text query.

OUTPUT TYPES:
    - TEXT: one plain string; collapse multiple spaces to one; preserve line breaks. Non-grounded output only.
    - TEXT2D: one plain string; preserve whitespace as layout cue (spaces + `\n` only; no coordinates). Non-grounded output only.
    - LINES: JSON array of objects, corresponding to line-by-line OCR: `{"text": string, "bbox": [x1,y1,x2,y2]}`. When locally reading, only return the text: `string`.
    - LATEX: JSON array of objects, corresponding to LaTeX expressions: `{"text": string, "bbox": [x1,y1,x2,y2]}`. When locally reading, only return the latex: `string`.
    - BOX: JSON array of bounding boxes only: `[ [x1,y1,x2,y2], ... ]`. For detection and conditional_detection tasks only.

When asked for OCR, always perform TEXT reading. No JSON, just plain text.
OCR=TEXT.
